# Emotion Recognition System

ðŸš€ Overview

This project aims to classify emotions from speech using deep learning techniques. The model uses audio features such as Mel-frequency cepstral coefficients (MFCCs) and spectrograms to predict emotions like happiness, sadness, anger, fear, and others.

The model is built using deep learning techniques that process audio files to extract relevant features and train a classifier. 

The key components of the project include:

MFCCs (Mel-frequency cepstral coefficients): A representation of the short-term power spectrum of sound.
Spectrograms: Visual representations of the spectrum of frequencies in a sound signal.

Key Features:
Speech-based Emotion Classification: Predicts emotions based on speech data.
Data Augmentation: Uses various techniques to enhance the dataset and improve model performance.
Training: Implements training scripts to develop a robust emotion recognition model.
Evaluation: Includes scripts to assess the modelâ€™s performance on unseen data.

## Project Directory Structure

```plaintext
Emotion-Recognition-System/
â”œâ”€â”€ data/                     # Folder for dataset
â”œâ”€â”€ models/                   # Folder for saving trained models
â”œâ”€â”€ notebooks/                # Jupyter Notebooks for analysis
â”œâ”€â”€ src/                      # Source code for preprocessing, training, evaluation
â”‚   â”œâ”€â”€ preprocessing.py      # Data preprocessing script
â”‚   â”œâ”€â”€ model.py              # Model definition script
â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â””â”€â”€ evaluate.py           # Evaluation script
â”œâ”€â”€ README.md                 # Project description and setup instructions
â””â”€â”€ requirements.txt          # Dependencies list
```
